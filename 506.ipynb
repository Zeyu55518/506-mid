{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660d699-89e2-401d-bbb9-08f6e6e4c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy import sparse\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "labeled = train.dropna(subset=[\"Score\"]).copy()\n",
    "test = train[train[\"Score\"].isna()].copy()\n",
    "\n",
    "# 合并 summary + reviewText\n",
    "labeled[\"text\"] = (labeled[\"summary\"].fillna('') + ' ' + labeled[\"reviewText\"].fillna('')).str.lower()\n",
    "test[\"text\"] = (test[\"summary\"].fillna('') + ' ' + test[\"reviewText\"].fillna('')).str.lower()\n",
    "\n",
    "# 划分训练 / 验证集\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    labeled, labeled[\"Score\"].astype(int),\n",
    "    test_size=0.2, random_state=42, stratify=labeled[\"Score\"]\n",
    ")\n",
    "X_te = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in [labeled, test]:\n",
    "    df[\"helpfulness_ratio\"] = np.where(df[\"TotalVotes\"] > 0,\n",
    "                                       df[\"VotedHelpful\"] / df[\"TotalVotes\"], 0)\n",
    "    df[\"helpfulness_log\"] = np.log1p(df[\"helpfulness_ratio\"])\n",
    "    df[\"review_length\"] = df[\"reviewText\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "    df[\"num_exclaim\"] = df[\"reviewText\"].fillna(\"\").apply(lambda x: x.count(\"!\"))\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "num_cols = [\"helpfulness_ratio\", \"helpfulness_log\", \"review_length\", \"num_exclaim\"]\n",
    "\n",
    "Xtr_num = scaler.fit_transform(labeled.loc[X_tr.index, num_cols])\n",
    "Xva_num = scaler.transform(labeled.loc[X_va.index, num_cols])\n",
    "Xte_num = scaler.transform(test[num_cols])\n",
    "\n",
    "print(\"Num shape:\", Xtr_num.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15addb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(genre):\n",
    "    \"\"\"提取单个 genre 字符串中的词\"\"\"\n",
    "    return re.findall(r\"[a-zA-Z]+\", str(genre).lower())\n",
    "\n",
    "def genre_to_tokens(genres):\n",
    "    \"\"\"将每个样本的所有 genre 合并为 token 集\"\"\"\n",
    "    tokens = []\n",
    "    if isinstance(genres, list):\n",
    "        for g in genres:\n",
    "            tokens.extend(get_tokens(g))\n",
    "    elif isinstance(genres, str):\n",
    "        tokens.extend(get_tokens(genres))\n",
    "    return list(set(tokens))  # 去重\n",
    "\n",
    "# ---- Step 1: 预处理所有数据 ----\n",
    "for df in [labeled, test]:\n",
    "    df[\"genres_list\"] = df[\"genres\"].fillna(\"\").apply(\n",
    "        lambda x: [g.strip() for g in x.split(\",\") if g.strip()]\n",
    "    )\n",
    "    df[\"genres_tokens\"] = df[\"genres_list\"].apply(genre_to_tokens)\n",
    "    df[\"genres_text\"] = df[\"genres_tokens\"].apply(lambda x: \" \".join(x))  # 转为字符串\n",
    "\n",
    "# ---- Step 2: TF-IDF 提取（1-2gram）----\n",
    "vec_genre = TfidfVectorizer(\n",
    "    max_features=2000,        # 你可以根据稀疏程度改为1000~3000\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "Xtr_genre = vec_genre.fit_transform(labeled.loc[X_tr.index, \"genres_text\"])\n",
    "Xva_genre = vec_genre.transform(labeled.loc[X_va.index, \"genres_text\"])\n",
    "Xte_genre = vec_genre.transform(test[\"genres_text\"])\n",
    "\n",
    "print(f\"Genre TF-IDF Shape: {Xtr_genre.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa937ab1-3290-48ec-82f7-f1c3095f112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(x):\n",
    "    return \" \".join(stemmer.stem(w) for w in x.split())\n",
    "\n",
    "X_tr_stem = X_tr[\"text\"].apply(stem_text)\n",
    "X_va_stem = X_va[\"text\"].apply(stem_text)\n",
    "X_te_stem = X_te[\"text\"].apply(stem_text)\n",
    "\n",
    "vec_word = TfidfVectorizer(max_features=50000, ngram_range=(1,4),\n",
    "                           min_df=2, max_df=0.99, sublinear_tf=True, stop_words='english')\n",
    "vec_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6),\n",
    "                           max_features=20000, min_df=3, binary=True)\n",
    "vec_stem = TfidfVectorizer(max_features=20000, ngram_range=(1,2),\n",
    "                           min_df=2, sublinear_tf=True)\n",
    "\n",
    "Xtr_tfidf = sparse.hstack([\n",
    "    vec_word.fit_transform(X_tr[\"text\"]),\n",
    "    vec_char.fit_transform(X_tr[\"text\"]),\n",
    "    vec_stem.fit_transform(X_tr_stem)\n",
    "])\n",
    "Xva_tfidf = sparse.hstack([\n",
    "    vec_word.transform(X_va[\"text\"]),\n",
    "    vec_char.transform(X_va[\"text\"]),\n",
    "    vec_stem.transform(X_va_stem)\n",
    "])\n",
    "Xte_tfidf = sparse.hstack([\n",
    "    vec_word.transform(X_te[\"text\"]),\n",
    "    vec_char.transform(X_te[\"text\"]),\n",
    "    vec_stem.transform(X_te_stem)\n",
    "])\n",
    "\n",
    "print(\"TF-IDF shape:\", Xtr_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import sparse\n",
    "\n",
    "sel = SelectKBest(chi2, k=15000)  \n",
    "Xtr_txt_sel = sel.fit_transform(Xtr_tfidf, y_tr)\n",
    "Xva_txt_sel = sel.transform(Xva_tfidf)\n",
    "Xte_txt_sel = sel.transform(Xte_tfidf)\n",
    "\n",
    "print(f\"txt Shape: {Xtr_txt_sel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入dense特征\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "Xtr_svd = svd.fit_transform(Xtr_tfidf)\n",
    "Xva_svd = svd.transform(Xva_tfidf)\n",
    "Xte_svd = svd.transform(Xte_tfidf)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # 注意 with_mean=False 适配稀疏拼接\n",
    "Xtr_num_scaled = scaler.fit_transform(Xtr_num)\n",
    "Xva_num_scaled = scaler.transform(Xva_num)\n",
    "Xte_num_scaled = scaler.transform(Xte_num)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526991cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_f = sparse.hstack([Xtr_txt_sel, Xtr_genre, sparse.csr_matrix(Xtr_num_scaled), sparse.csr_matrix(Xtr_svd)], format='csr')\n",
    "Xva_f = sparse.hstack([Xva_txt_sel, Xva_genre, sparse.csr_matrix(Xva_num_scaled), sparse.csr_matrix(Xva_svd)], format='csr')\n",
    "Xte_f = sparse.hstack([Xte_txt_sel, Xte_genre, sparse.csr_matrix(Xte_num_scaled),sparse.csr_matrix(Xte_svd)], format='csr')\n",
    "print(\"Final train feature shape:\", Xtr_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ee889-093d-4c4c-97bc-f8205d265499",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    solver='lbfgs',\n",
    "    C=0.2,\n",
    "    multi_class='multinomial',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr.fit(Xtr_f, y_tr)\n",
    "pred = lr.predict(Xva_f)\n",
    "\n",
    "print(f\"\\nMacro F1: {f1_score(y_va, pred, average='macro'):.4f}\")\n",
    "print(classification_report(y_va, pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(Xtr_tfidf, y_tr)\n",
    "probs_cnb_tr = cnb.predict_proba(Xtr_tfidf)\n",
    "probs_cnb_va = cnb.predict_proba(Xva_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed026b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_tr = lr.predict_proba(Xtr_f)\n",
    "probs_va = lr.predict_proba(Xva_f)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "probs_tr_scaled = scaler.fit_transform(probs_tr)\n",
    "probs_va_scaled = scaler.transform(probs_va)\n",
    "\n",
    "\n",
    "Xtr_stack = sparse.hstack([Xtr_f, probs_tr_scaled,probs_cnb_tr])\n",
    "Xva_stack = sparse.hstack([Xva_f, probs_va_scaled,probs_cnb_va])\n",
    "\n",
    "meta = LogisticRegression(C=0.2, solver='lbfgs', multi_class='multinomial', class_weight='balanced', max_iter=2000)\n",
    "meta.fit(Xtr_stack, y_tr)\n",
    "pred_meta = meta.predict(Xva_stack)\n",
    "print(f\"Stacked F1: {f1_score(y_va, pred_meta, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classes = meta.classes_\n",
    "print(\"Class order:\", classes)\n",
    "\n",
    "proba = meta.predict_proba(Xva_stack) + 1e-12\n",
    "logp = np.log(proba)\n",
    "\n",
    "bias_grid = np.linspace(-0.3, 0.3, 13)\n",
    "print(f\"Grid size per dim: {len(bias_grid)}\")\n",
    "print(f\"Total combinations: {len(bias_grid)**5:,}\")\n",
    "\n",
    "def eval_bias(B):\n",
    "    p = np.exp(logp + B)\n",
    "    p /= p.sum(axis=1, keepdims=True)\n",
    "    pred = p.argmax(axis=1) + classes.min()\n",
    "    f1 = f1_score(y_va, pred, average='macro')\n",
    "    return f1, B\n",
    "\n",
    "bias_list = [\n",
    "    np.array([b1, b2, b3, b4, b5])\n",
    "    for b1 in bias_grid\n",
    "    for b2 in bias_grid\n",
    "    for b3 in bias_grid\n",
    "    for b4 in bias_grid\n",
    "    for b5 in bias_grid\n",
    "]\n",
    "\n",
    "results = Parallel(n_jobs=32, backend='loky', verbose=10)(\n",
    "    delayed(eval_bias)(B) for B in bias_list\n",
    ")\n",
    "\n",
    "best_f1, best_B = max(results, key=lambda x: x[0])\n",
    "print(f\"\\n✅ Best Macro-F1: {best_f1:.6f}\")\n",
    "print(\"Best bias:\", best_B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc342258",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_te = lr.predict_proba(Xte_f)\n",
    "probs_te_scaled = scaler.transform(probs_te)\n",
    "probs_cnb_te = cnb.predict_proba(Xte_tfidf)\n",
    "Xte_stack = sparse.hstack([Xte_f, probs_te_scaled,probs_cnb_te])\n",
    "\n",
    "test_pred = meta.predict(Xte_stack)\n",
    "submission = pd.DataFrame({'id': test['id'], 'Score': test_pred})\n",
    "submission.to_csv('submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245cb17-0394-4984-a0c2-418b59be3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 预测 ----------\n",
    "\n",
    "probs_te = meta.predict_proba(Xte_stack) + 1e-12\n",
    "\n",
    "# ---------- 应用 log-bias ----------\n",
    "best_bias = np.array(best_B)  # 来自你上面最优结果\n",
    "logp = np.log(probs_te)\n",
    "p_adj = np.exp(logp + best_bias)\n",
    "p_adj = p_adj / p_adj.sum(axis=1, keepdims=True)  # 归一化为概率\n",
    "\n",
    "# ---------- 得出最终预测 ----------\n",
    "test_pred = p_adj.argmax(axis=1) + 1  # 类别从1开始\n",
    "\n",
    "# ---------- 保存结果 ----------\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Score': test_pred\n",
    "})\n",
    "submission.to_csv('submission_f.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
